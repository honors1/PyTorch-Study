{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitpytorchcondaa830780c895249409727a78a8b33e785",
   "display_name": "Python 3.7.9 64-bit ('PyTorch': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faf29d94230>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], \n",
    "                              [93], \n",
    "                              [89], \n",
    "                              [96], \n",
    "                              [73]])\n",
    "x2_train = torch.FloatTensor([[80], \n",
    "                              [88], \n",
    "                              [91], \n",
    "                              [98], \n",
    "                              [66]])\n",
    "x3_train = torch.FloatTensor([[ 75], \n",
    "                              [ 93], \n",
    "                              [ 90], \n",
    "                              [100], \n",
    "                              [ 70]])\n",
    "y_train = torch.FloatTensor([[152], \n",
    "                             [185], \n",
    "                             [180], \n",
    "                             [196], \n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 00000/10000, w1: 0.2940, w2: 0.2936, w3: 0.2974, b: 0.0034, Cost: 29661.8007812500\n",
      "Epoch 00100/10000, w1: 0.6735, w2: 0.6610, w3: 0.6762, b: 0.0079, Cost: 01.5636277199\n",
      "Epoch 00200/10000, w1: 0.6789, w2: 0.6550, w3: 0.6768, b: 0.0081, Cost: 01.4975947142\n",
      "Epoch 00300/10000, w1: 0.6843, w2: 0.6491, w3: 0.6773, b: 0.0082, Cost: 01.4350442886\n",
      "Epoch 00400/10000, w1: 0.6894, w2: 0.6434, w3: 0.6778, b: 0.0084, Cost: 01.3757257462\n",
      "Epoch 00500/10000, w1: 0.6945, w2: 0.6379, w3: 0.6783, b: 0.0085, Cost: 01.3194968700\n",
      "Epoch 00600/10000, w1: 0.6994, w2: 0.6326, w3: 0.6787, b: 0.0087, Cost: 01.2662148476\n",
      "Epoch 00700/10000, w1: 0.7042, w2: 0.6273, w3: 0.6791, b: 0.0088, Cost: 01.2157032490\n",
      "Epoch 00800/10000, w1: 0.7089, w2: 0.6223, w3: 0.6795, b: 0.0089, Cost: 01.1678097248\n",
      "Epoch 00900/10000, w1: 0.7135, w2: 0.6173, w3: 0.6798, b: 0.0091, Cost: 01.1224288940\n",
      "Epoch 01000/10000, w1: 0.7179, w2: 0.6125, w3: 0.6801, b: 0.0092, Cost: 01.0793898106\n",
      "Epoch 01100/10000, w1: 0.7223, w2: 0.6079, w3: 0.6804, b: 0.0094, Cost: 01.0385738611\n",
      "Epoch 01200/10000, w1: 0.7265, w2: 0.6033, w3: 0.6807, b: 0.0095, Cost: 00.9999003410\n",
      "Epoch 01300/10000, w1: 0.7307, w2: 0.5989, w3: 0.6810, b: 0.0097, Cost: 00.9632169604\n",
      "Epoch 01400/10000, w1: 0.7347, w2: 0.5947, w3: 0.6812, b: 0.0098, Cost: 00.9284214973\n",
      "Epoch 01500/10000, w1: 0.7387, w2: 0.5905, w3: 0.6814, b: 0.0099, Cost: 00.8954483867\n",
      "Epoch 01600/10000, w1: 0.7425, w2: 0.5865, w3: 0.6816, b: 0.0101, Cost: 00.8641813993\n",
      "Epoch 01700/10000, w1: 0.7463, w2: 0.5825, w3: 0.6817, b: 0.0102, Cost: 00.8345165253\n",
      "Epoch 01800/10000, w1: 0.7500, w2: 0.5787, w3: 0.6819, b: 0.0103, Cost: 00.8063680530\n",
      "Epoch 01900/10000, w1: 0.7535, w2: 0.5750, w3: 0.6820, b: 0.0105, Cost: 00.7796915174\n",
      "Epoch 02000/10000, w1: 0.7570, w2: 0.5714, w3: 0.6821, b: 0.0106, Cost: 00.7543791533\n",
      "Epoch 02100/10000, w1: 0.7605, w2: 0.5679, w3: 0.6822, b: 0.0107, Cost: 00.7303718328\n",
      "Epoch 02200/10000, w1: 0.7638, w2: 0.5645, w3: 0.6822, b: 0.0109, Cost: 00.7075891495\n",
      "Epoch 02300/10000, w1: 0.7671, w2: 0.5612, w3: 0.6823, b: 0.0110, Cost: 00.6859962344\n",
      "Epoch 02400/10000, w1: 0.7702, w2: 0.5580, w3: 0.6823, b: 0.0111, Cost: 00.6654896140\n",
      "Epoch 02500/10000, w1: 0.7734, w2: 0.5548, w3: 0.6823, b: 0.0112, Cost: 00.6460350156\n",
      "Epoch 02600/10000, w1: 0.7764, w2: 0.5518, w3: 0.6823, b: 0.0114, Cost: 00.6275782585\n",
      "Epoch 02700/10000, w1: 0.7794, w2: 0.5488, w3: 0.6823, b: 0.0115, Cost: 00.6100503206\n",
      "Epoch 02800/10000, w1: 0.7823, w2: 0.5460, w3: 0.6822, b: 0.0116, Cost: 00.5934207439\n",
      "Epoch 02900/10000, w1: 0.7851, w2: 0.5432, w3: 0.6822, b: 0.0118, Cost: 00.5776335001\n",
      "Epoch 03000/10000, w1: 0.7879, w2: 0.5405, w3: 0.6821, b: 0.0119, Cost: 00.5626521707\n",
      "Epoch 03100/10000, w1: 0.7906, w2: 0.5379, w3: 0.6820, b: 0.0120, Cost: 00.5484347343\n",
      "Epoch 03200/10000, w1: 0.7932, w2: 0.5353, w3: 0.6819, b: 0.0121, Cost: 00.5349179506\n",
      "Epoch 03300/10000, w1: 0.7958, w2: 0.5328, w3: 0.6818, b: 0.0123, Cost: 00.5220851302\n",
      "Epoch 03400/10000, w1: 0.7983, w2: 0.5304, w3: 0.6817, b: 0.0124, Cost: 00.5099018812\n",
      "Epoch 03500/10000, w1: 0.8008, w2: 0.5281, w3: 0.6816, b: 0.0125, Cost: 00.4983332753\n",
      "Epoch 03600/10000, w1: 0.8032, w2: 0.5258, w3: 0.6814, b: 0.0126, Cost: 00.4873361588\n",
      "Epoch 03700/10000, w1: 0.8055, w2: 0.5236, w3: 0.6813, b: 0.0127, Cost: 00.4768794179\n",
      "Epoch 03800/10000, w1: 0.8078, w2: 0.5215, w3: 0.6811, b: 0.0129, Cost: 00.4669612944\n",
      "Epoch 03900/10000, w1: 0.8101, w2: 0.5194, w3: 0.6809, b: 0.0130, Cost: 00.4575297832\n",
      "Epoch 04000/10000, w1: 0.8123, w2: 0.5174, w3: 0.6807, b: 0.0131, Cost: 00.4485570788\n",
      "Epoch 04100/10000, w1: 0.8145, w2: 0.5155, w3: 0.6805, b: 0.0132, Cost: 00.4400365949\n",
      "Epoch 04200/10000, w1: 0.8166, w2: 0.5136, w3: 0.6803, b: 0.0133, Cost: 00.4319244027\n",
      "Epoch 04300/10000, w1: 0.8186, w2: 0.5117, w3: 0.6801, b: 0.0135, Cost: 00.4242107868\n",
      "Epoch 04400/10000, w1: 0.8206, w2: 0.5099, w3: 0.6799, b: 0.0136, Cost: 00.4168747067\n",
      "Epoch 04500/10000, w1: 0.8226, w2: 0.5082, w3: 0.6796, b: 0.0137, Cost: 00.4099035859\n",
      "Epoch 04600/10000, w1: 0.8246, w2: 0.5065, w3: 0.6794, b: 0.0138, Cost: 00.4032565057\n",
      "Epoch 04700/10000, w1: 0.8264, w2: 0.5049, w3: 0.6791, b: 0.0139, Cost: 00.3969356120\n",
      "Epoch 04800/10000, w1: 0.8283, w2: 0.5033, w3: 0.6789, b: 0.0140, Cost: 00.3909222186\n",
      "Epoch 04900/10000, w1: 0.8301, w2: 0.5018, w3: 0.6786, b: 0.0142, Cost: 00.3851898313\n",
      "Epoch 05000/10000, w1: 0.8319, w2: 0.5003, w3: 0.6783, b: 0.0143, Cost: 00.3797388673\n",
      "Epoch 05100/10000, w1: 0.8336, w2: 0.4988, w3: 0.6780, b: 0.0144, Cost: 00.3745294511\n",
      "Epoch 05200/10000, w1: 0.8353, w2: 0.4974, w3: 0.6777, b: 0.0145, Cost: 00.3695805073\n",
      "Epoch 05300/10000, w1: 0.8370, w2: 0.4961, w3: 0.6774, b: 0.0146, Cost: 00.3648600280\n",
      "Epoch 05400/10000, w1: 0.8386, w2: 0.4947, w3: 0.6771, b: 0.0147, Cost: 00.3603575230\n",
      "Epoch 05500/10000, w1: 0.8402, w2: 0.4935, w3: 0.6768, b: 0.0148, Cost: 00.3560605943\n",
      "Epoch 05600/10000, w1: 0.8418, w2: 0.4922, w3: 0.6765, b: 0.0149, Cost: 00.3519615531\n",
      "Epoch 05700/10000, w1: 0.8433, w2: 0.4910, w3: 0.6761, b: 0.0151, Cost: 00.3480525613\n",
      "Epoch 05800/10000, w1: 0.8448, w2: 0.4899, w3: 0.6758, b: 0.0152, Cost: 00.3443239331\n",
      "Epoch 05900/10000, w1: 0.8463, w2: 0.4887, w3: 0.6755, b: 0.0153, Cost: 00.3407582939\n",
      "Epoch 06000/10000, w1: 0.8477, w2: 0.4877, w3: 0.6751, b: 0.0154, Cost: 00.3373603523\n",
      "Epoch 06100/10000, w1: 0.8491, w2: 0.4866, w3: 0.6748, b: 0.0155, Cost: 00.3341016173\n",
      "Epoch 06200/10000, w1: 0.8505, w2: 0.4856, w3: 0.6744, b: 0.0156, Cost: 00.3309959173\n",
      "Epoch 06300/10000, w1: 0.8519, w2: 0.4846, w3: 0.6740, b: 0.0157, Cost: 00.3280245364\n",
      "Epoch 06400/10000, w1: 0.8532, w2: 0.4836, w3: 0.6737, b: 0.0158, Cost: 00.3251844943\n",
      "Epoch 06500/10000, w1: 0.8545, w2: 0.4827, w3: 0.6733, b: 0.0159, Cost: 00.3224549890\n",
      "Epoch 06600/10000, w1: 0.8558, w2: 0.4818, w3: 0.6729, b: 0.0160, Cost: 00.3198528886\n",
      "Epoch 06700/10000, w1: 0.8570, w2: 0.4809, w3: 0.6725, b: 0.0162, Cost: 00.3173592091\n",
      "Epoch 06800/10000, w1: 0.8583, w2: 0.4801, w3: 0.6721, b: 0.0163, Cost: 00.3149678707\n",
      "Epoch 06900/10000, w1: 0.8595, w2: 0.4793, w3: 0.6717, b: 0.0164, Cost: 00.3126714528\n",
      "Epoch 07000/10000, w1: 0.8607, w2: 0.4785, w3: 0.6714, b: 0.0165, Cost: 00.3104756474\n",
      "Epoch 07100/10000, w1: 0.8618, w2: 0.4777, w3: 0.6710, b: 0.0166, Cost: 00.3083655238\n",
      "Epoch 07200/10000, w1: 0.8630, w2: 0.4770, w3: 0.6706, b: 0.0167, Cost: 00.3063442409\n",
      "Epoch 07300/10000, w1: 0.8641, w2: 0.4763, w3: 0.6701, b: 0.0168, Cost: 00.3043988347\n",
      "Epoch 07400/10000, w1: 0.8652, w2: 0.4756, w3: 0.6697, b: 0.0169, Cost: 00.3025324345\n",
      "Epoch 07500/10000, w1: 0.8663, w2: 0.4749, w3: 0.6693, b: 0.0170, Cost: 00.3007273376\n",
      "Epoch 07600/10000, w1: 0.8673, w2: 0.4743, w3: 0.6689, b: 0.0171, Cost: 00.2990024090\n",
      "Epoch 07700/10000, w1: 0.8684, w2: 0.4736, w3: 0.6685, b: 0.0172, Cost: 00.2973411977\n",
      "Epoch 07800/10000, w1: 0.8694, w2: 0.4730, w3: 0.6681, b: 0.0173, Cost: 00.2957390249\n",
      "Epoch 07900/10000, w1: 0.8704, w2: 0.4725, w3: 0.6676, b: 0.0174, Cost: 00.2942033112\n",
      "Epoch 08000/10000, w1: 0.8714, w2: 0.4719, w3: 0.6672, b: 0.0175, Cost: 00.2927091718\n",
      "Epoch 08100/10000, w1: 0.8724, w2: 0.4714, w3: 0.6668, b: 0.0176, Cost: 00.2912767231\n",
      "Epoch 08200/10000, w1: 0.8733, w2: 0.4708, w3: 0.6664, b: 0.0177, Cost: 00.2898924947\n",
      "Epoch 08300/10000, w1: 0.8743, w2: 0.4703, w3: 0.6659, b: 0.0178, Cost: 00.2885522544\n",
      "Epoch 08400/10000, w1: 0.8752, w2: 0.4698, w3: 0.6655, b: 0.0179, Cost: 00.2872648537\n",
      "Epoch 08500/10000, w1: 0.8761, w2: 0.4694, w3: 0.6650, b: 0.0180, Cost: 00.2860192657\n",
      "Epoch 08600/10000, w1: 0.8770, w2: 0.4689, w3: 0.6646, b: 0.0181, Cost: 00.2848047912\n",
      "Epoch 08700/10000, w1: 0.8779, w2: 0.4685, w3: 0.6642, b: 0.0182, Cost: 00.2836406231\n",
      "Epoch 08800/10000, w1: 0.8787, w2: 0.4681, w3: 0.6637, b: 0.0183, Cost: 00.2825062275\n",
      "Epoch 08900/10000, w1: 0.8796, w2: 0.4677, w3: 0.6633, b: 0.0184, Cost: 00.2814104855\n",
      "Epoch 09000/10000, w1: 0.8804, w2: 0.4673, w3: 0.6628, b: 0.0185, Cost: 00.2803494334\n",
      "Epoch 09100/10000, w1: 0.8813, w2: 0.4669, w3: 0.6624, b: 0.0186, Cost: 00.2793147862\n",
      "Epoch 09200/10000, w1: 0.8821, w2: 0.4666, w3: 0.6619, b: 0.0187, Cost: 00.2783156931\n",
      "Epoch 09300/10000, w1: 0.8829, w2: 0.4662, w3: 0.6615, b: 0.0188, Cost: 00.2773452997\n",
      "Epoch 09400/10000, w1: 0.8837, w2: 0.4659, w3: 0.6610, b: 0.0189, Cost: 00.2763910294\n",
      "Epoch 09500/10000, w1: 0.8844, w2: 0.4656, w3: 0.6605, b: 0.0190, Cost: 00.2754762769\n",
      "Epoch 09600/10000, w1: 0.8852, w2: 0.4653, w3: 0.6601, b: 0.0191, Cost: 00.2745783925\n",
      "Epoch 09700/10000, w1: 0.8860, w2: 0.4650, w3: 0.6596, b: 0.0192, Cost: 00.2737014890\n",
      "Epoch 09800/10000, w1: 0.8867, w2: 0.4647, w3: 0.6592, b: 0.0193, Cost: 00.2728615403\n",
      "Epoch 09900/10000, w1: 0.8874, w2: 0.4645, w3: 0.6587, b: 0.0194, Cost: 00.2720293999\n",
      "Epoch 10000/10000, w1: 0.8881, w2: 0.4642, w3: 0.6582, b: 0.0195, Cost: 00.2712195218\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 10000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3 + b\n",
    "\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:05d}/{nb_epochs}, w1: {w1.item():.4f}, w2: {w2.item():.4f}, w3: {w3.item():.4f}, b: {b.item():.4f}, Cost: {cost.item():013.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,   75], \n",
    "                               [93,  88,   93], \n",
    "                               [89,  91,   90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,   70]])  \n",
    "                               \n",
    "y_train  =  torch.FloatTensor([[152],  \n",
    "                               [185],  \n",
    "                               [180],  \n",
    "                               [196],  \n",
    "                               [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of x_train: torch.Size([5, 3])\nShape of y_train: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0000/20000, hypothesis: tensor([0., 0., 0., 0., 0.]), Cost: 29661.8007812500\n",
      "Epoch 0100/20000, hypothesis: tensor([152.7691, 183.6985, 180.9591, 197.0627, 140.1336]), Cost: 01.5636277199\n",
      "Epoch 0200/20000, hypothesis: tensor([152.7273, 183.7273, 180.9465, 197.0517, 140.1731]), Cost: 01.4975947142\n",
      "Epoch 0300/20000, hypothesis: tensor([152.6866, 183.7554, 180.9343, 197.0409, 140.2116]), Cost: 01.4350442886\n",
      "Epoch 0400/20000, hypothesis: tensor([152.6470, 183.7827, 180.9224, 197.0304, 140.2491]), Cost: 01.3757257462\n",
      "Epoch 0500/20000, hypothesis: tensor([152.6085, 183.8093, 180.9108, 197.0201, 140.2856]), Cost: 01.3194968700\n",
      "Epoch 0600/20000, hypothesis: tensor([152.5711, 183.8352, 180.8996, 197.0101, 140.3211]), Cost: 01.2662148476\n",
      "Epoch 0700/20000, hypothesis: tensor([152.5346, 183.8604, 180.8887, 197.0003, 140.3557]), Cost: 01.2157032490\n",
      "Epoch 0800/20000, hypothesis: tensor([152.4992, 183.8849, 180.8780, 196.9908, 140.3895]), Cost: 01.1678097248\n",
      "Epoch 0900/20000, hypothesis: tensor([152.4647, 183.9087, 180.8677, 196.9814, 140.4223]), Cost: 01.1224288940\n",
      "Epoch 1000/20000, hypothesis: tensor([152.4312, 183.9319, 180.8577, 196.9723, 140.4543]), Cost: 01.0793898106\n",
      "Epoch 1100/20000, hypothesis: tensor([152.3986, 183.9544, 180.8480, 196.9633, 140.4855]), Cost: 01.0385738611\n",
      "Epoch 1200/20000, hypothesis: tensor([152.3669, 183.9763, 180.8385, 196.9546, 140.5159]), Cost: 00.9999003410\n",
      "Epoch 1300/20000, hypothesis: tensor([152.3360, 183.9977, 180.8293, 196.9461, 140.5454]), Cost: 00.9632169604\n",
      "Epoch 1400/20000, hypothesis: tensor([152.3060, 184.0184, 180.8203, 196.9377, 140.5743]), Cost: 00.9284214973\n",
      "Epoch 1500/20000, hypothesis: tensor([152.2769, 184.0386, 180.8116, 196.9296, 140.6023]), Cost: 00.8954483867\n",
      "Epoch 1600/20000, hypothesis: tensor([152.2485, 184.0583, 180.8031, 196.9216, 140.6297]), Cost: 00.8641813993\n",
      "Epoch 1700/20000, hypothesis: tensor([152.2209, 184.0773, 180.7949, 196.9138, 140.6563]), Cost: 00.8345165253\n",
      "Epoch 1800/20000, hypothesis: tensor([152.1940, 184.0959, 180.7869, 196.9062, 140.6823]), Cost: 00.8063680530\n",
      "Epoch 1900/20000, hypothesis: tensor([152.1679, 184.1140, 180.7792, 196.8988, 140.7076]), Cost: 00.7796915174\n",
      "Epoch 2000/20000, hypothesis: tensor([152.1425, 184.1316, 180.7716, 196.8914, 140.7322]), Cost: 00.7543791533\n",
      "Epoch 2100/20000, hypothesis: tensor([152.1178, 184.1487, 180.7643, 196.8843, 140.7562]), Cost: 00.7303718328\n",
      "Epoch 2200/20000, hypothesis: tensor([152.0939, 184.1654, 180.7572, 196.8773, 140.7796]), Cost: 00.7075891495\n",
      "Epoch 2300/20000, hypothesis: tensor([152.0705, 184.1815, 180.7502, 196.8705, 140.8023]), Cost: 00.6859962344\n",
      "Epoch 2400/20000, hypothesis: tensor([152.0478, 184.1973, 180.7435, 196.8639, 140.8245]), Cost: 00.6654896140\n",
      "Epoch 2500/20000, hypothesis: tensor([152.0257, 184.2126, 180.7370, 196.8573, 140.8461]), Cost: 00.6460350156\n",
      "Epoch 2600/20000, hypothesis: tensor([152.0043, 184.2275, 180.7307, 196.8509, 140.8672]), Cost: 00.6275782585\n",
      "Epoch 2700/20000, hypothesis: tensor([151.9834, 184.2420, 180.7245, 196.8447, 140.8878]), Cost: 00.6100503206\n",
      "Epoch 2800/20000, hypothesis: tensor([151.9631, 184.2561, 180.7185, 196.8385, 140.9078]), Cost: 00.5934207439\n",
      "Epoch 2900/20000, hypothesis: tensor([151.9434, 184.2698, 180.7127, 196.8325, 140.9273]), Cost: 00.5776335001\n",
      "Epoch 3000/20000, hypothesis: tensor([151.9242, 184.2831, 180.7070, 196.8267, 140.9463]), Cost: 00.5626521707\n",
      "Epoch 3100/20000, hypothesis: tensor([151.9056, 184.2961, 180.7015, 196.8209, 140.9648]), Cost: 00.5484347343\n",
      "Epoch 3200/20000, hypothesis: tensor([151.8875, 184.3087, 180.6962, 196.8153, 140.9828]), Cost: 00.5349179506\n",
      "Epoch 3300/20000, hypothesis: tensor([151.8698, 184.3209, 180.6910, 196.8098, 141.0004]), Cost: 00.5220851302\n",
      "Epoch 3400/20000, hypothesis: tensor([151.8527, 184.3329, 180.6860, 196.8044, 141.0176]), Cost: 00.5099018812\n",
      "Epoch 3500/20000, hypothesis: tensor([151.8360, 184.3445, 180.6811, 196.7991, 141.0343]), Cost: 00.4983332753\n",
      "Epoch 3600/20000, hypothesis: tensor([151.8199, 184.3557, 180.6764, 196.7939, 141.0506]), Cost: 00.4873361588\n",
      "Epoch 3700/20000, hypothesis: tensor([151.8041, 184.3667, 180.6718, 196.7888, 141.0665]), Cost: 00.4768794179\n",
      "Epoch 3800/20000, hypothesis: tensor([151.7888, 184.3774, 180.6673, 196.7838, 141.0820]), Cost: 00.4669612944\n",
      "Epoch 3900/20000, hypothesis: tensor([151.7740, 184.3877, 180.6630, 196.7789, 141.0970]), Cost: 00.4575297832\n",
      "Epoch 4000/20000, hypothesis: tensor([151.7595, 184.3978, 180.6588, 196.7742, 141.1117]), Cost: 00.4485570788\n",
      "Epoch 4100/20000, hypothesis: tensor([151.7455, 184.4076, 180.6547, 196.7695, 141.1261]), Cost: 00.4400365949\n",
      "Epoch 4200/20000, hypothesis: tensor([151.7318, 184.4172, 180.6508, 196.7649, 141.1401]), Cost: 00.4319244027\n",
      "Epoch 4300/20000, hypothesis: tensor([151.7185, 184.4264, 180.6469, 196.7603, 141.1537]), Cost: 00.4242107868\n",
      "Epoch 4400/20000, hypothesis: tensor([151.7057, 184.4355, 180.6432, 196.7559, 141.1670]), Cost: 00.4168747067\n",
      "Epoch 4500/20000, hypothesis: tensor([151.6931, 184.4442, 180.6396, 196.7516, 141.1799]), Cost: 00.4099035859\n",
      "Epoch 4600/20000, hypothesis: tensor([151.6810, 184.4527, 180.6361, 196.7473, 141.1926]), Cost: 00.4032565057\n",
      "Epoch 4700/20000, hypothesis: tensor([151.6691, 184.4610, 180.6326, 196.7431, 141.2049]), Cost: 00.3969356120\n",
      "Epoch 4800/20000, hypothesis: tensor([151.6576, 184.4691, 180.6293, 196.7390, 141.2169]), Cost: 00.3909222186\n",
      "Epoch 4900/20000, hypothesis: tensor([151.6465, 184.4769, 180.6262, 196.7350, 141.2287]), Cost: 00.3851898313\n",
      "Epoch 5000/20000, hypothesis: tensor([151.6356, 184.4845, 180.6230, 196.7311, 141.2401]), Cost: 00.3797388673\n",
      "Epoch 5100/20000, hypothesis: tensor([151.6251, 184.4919, 180.6200, 196.7272, 141.2512]), Cost: 00.3745294511\n",
      "Epoch 5200/20000, hypothesis: tensor([151.6148, 184.4991, 180.6171, 196.7234, 141.2621]), Cost: 00.3695805073\n",
      "Epoch 5300/20000, hypothesis: tensor([151.6049, 184.5061, 180.6143, 196.7197, 141.2727]), Cost: 00.3648600280\n",
      "Epoch 5400/20000, hypothesis: tensor([151.5952, 184.5129, 180.6115, 196.7160, 141.2830]), Cost: 00.3603575230\n",
      "Epoch 5500/20000, hypothesis: tensor([151.5858, 184.5195, 180.6088, 196.7124, 141.2931]), Cost: 00.3560605943\n",
      "Epoch 5600/20000, hypothesis: tensor([151.5767, 184.5259, 180.6063, 196.7088, 141.3030]), Cost: 00.3519615531\n",
      "Epoch 5700/20000, hypothesis: tensor([151.5678, 184.5322, 180.6038, 196.7053, 141.3126]), Cost: 00.3480525613\n",
      "Epoch 5800/20000, hypothesis: tensor([151.5592, 184.5382, 180.6013, 196.7019, 141.3219]), Cost: 00.3443239331\n",
      "Epoch 5900/20000, hypothesis: tensor([151.5509, 184.5441, 180.5990, 196.6986, 141.3311]), Cost: 00.3407582939\n",
      "Epoch 6000/20000, hypothesis: tensor([151.5428, 184.5498, 180.5967, 196.6953, 141.3400]), Cost: 00.3373603523\n",
      "Epoch 6100/20000, hypothesis: tensor([151.5349, 184.5554, 180.5945, 196.6920, 141.3487]), Cost: 00.3341016173\n",
      "Epoch 6200/20000, hypothesis: tensor([151.5273, 184.5608, 180.5924, 196.6888, 141.3572]), Cost: 00.3309959173\n",
      "Epoch 6300/20000, hypothesis: tensor([151.5199, 184.5661, 180.5903, 196.6857, 141.3655]), Cost: 00.3280245364\n",
      "Epoch 6400/20000, hypothesis: tensor([151.5126, 184.5712, 180.5883, 196.6826, 141.3736]), Cost: 00.3251844943\n",
      "Epoch 6500/20000, hypothesis: tensor([151.5056, 184.5761, 180.5864, 196.6795, 141.3815]), Cost: 00.3224549890\n",
      "Epoch 6600/20000, hypothesis: tensor([151.4989, 184.5809, 180.5845, 196.6766, 141.3892]), Cost: 00.3198528886\n",
      "Epoch 6700/20000, hypothesis: tensor([151.4923, 184.5856, 180.5827, 196.6736, 141.3967]), Cost: 00.3173592091\n",
      "Epoch 6800/20000, hypothesis: tensor([151.4859, 184.5902, 180.5810, 196.6707, 141.4041]), Cost: 00.3149678707\n",
      "Epoch 6900/20000, hypothesis: tensor([151.4797, 184.5946, 180.5793, 196.6679, 141.4112]), Cost: 00.3126714528\n",
      "Epoch 7000/20000, hypothesis: tensor([151.4737, 184.5989, 180.5777, 196.6651, 141.4182]), Cost: 00.3104756474\n",
      "Epoch 7100/20000, hypothesis: tensor([151.4678, 184.6031, 180.5761, 196.6623, 141.4251]), Cost: 00.3083655238\n",
      "Epoch 7200/20000, hypothesis: tensor([151.4621, 184.6071, 180.5745, 196.6596, 141.4317]), Cost: 00.3063442409\n",
      "Epoch 7300/20000, hypothesis: tensor([151.4566, 184.6110, 180.5730, 196.6569, 141.4383]), Cost: 00.3043988347\n",
      "Epoch 7400/20000, hypothesis: tensor([151.4513, 184.6149, 180.5716, 196.6543, 141.4446]), Cost: 00.3025324345\n",
      "Epoch 7500/20000, hypothesis: tensor([151.4462, 184.6186, 180.5702, 196.6517, 141.4509]), Cost: 00.3007273376\n",
      "Epoch 7600/20000, hypothesis: tensor([151.4411, 184.6222, 180.5689, 196.6491, 141.4569]), Cost: 00.2990024090\n",
      "Epoch 7700/20000, hypothesis: tensor([151.4363, 184.6257, 180.5676, 196.6466, 141.4629]), Cost: 00.2973411977\n",
      "Epoch 7800/20000, hypothesis: tensor([151.4315, 184.6291, 180.5664, 196.6441, 141.4687]), Cost: 00.2957390249\n",
      "Epoch 7900/20000, hypothesis: tensor([151.4270, 184.6324, 180.5652, 196.6417, 141.4743]), Cost: 00.2942033112\n",
      "Epoch 8000/20000, hypothesis: tensor([151.4225, 184.6355, 180.5640, 196.6393, 141.4799]), Cost: 00.2927091718\n",
      "Epoch 8100/20000, hypothesis: tensor([151.4182, 184.6386, 180.5629, 196.6369, 141.4853]), Cost: 00.2912767231\n",
      "Epoch 8200/20000, hypothesis: tensor([151.4141, 184.6417, 180.5618, 196.6345, 141.4906]), Cost: 00.2898924947\n",
      "Epoch 8300/20000, hypothesis: tensor([151.4100, 184.6446, 180.5608, 196.6322, 141.4957]), Cost: 00.2885522544\n",
      "Epoch 8400/20000, hypothesis: tensor([151.4061, 184.6474, 180.5598, 196.6299, 141.5008]), Cost: 00.2872648537\n",
      "Epoch 8500/20000, hypothesis: tensor([151.4023, 184.6502, 180.5588, 196.6277, 141.5057]), Cost: 00.2860192657\n",
      "Epoch 8600/20000, hypothesis: tensor([151.3987, 184.6529, 180.5579, 196.6254, 141.5106]), Cost: 00.2848047912\n",
      "Epoch 8700/20000, hypothesis: tensor([151.3951, 184.6554, 180.5570, 196.6232, 141.5153]), Cost: 00.2836406231\n",
      "Epoch 8800/20000, hypothesis: tensor([151.3917, 184.6580, 180.5561, 196.6210, 141.5199]), Cost: 00.2825062275\n",
      "Epoch 8900/20000, hypothesis: tensor([151.3884, 184.6604, 180.5553, 196.6189, 141.5244]), Cost: 00.2814104855\n",
      "Epoch 9000/20000, hypothesis: tensor([151.3851, 184.6628, 180.5545, 196.6168, 141.5288]), Cost: 00.2803494334\n",
      "Epoch 9100/20000, hypothesis: tensor([151.3820, 184.6651, 180.5538, 196.6147, 141.5331]), Cost: 00.2793147862\n",
      "Epoch 9200/20000, hypothesis: tensor([151.3790, 184.6673, 180.5530, 196.6126, 141.5374]), Cost: 00.2783156931\n",
      "Epoch 9300/20000, hypothesis: tensor([151.3761, 184.6694, 180.5523, 196.6106, 141.5415]), Cost: 00.2773452997\n",
      "Epoch 9400/20000, hypothesis: tensor([151.3732, 184.6716, 180.5517, 196.6086, 141.5455]), Cost: 00.2763910294\n",
      "Epoch 9500/20000, hypothesis: tensor([151.3705, 184.6736, 180.5510, 196.6066, 141.5495]), Cost: 00.2754762769\n",
      "Epoch 9600/20000, hypothesis: tensor([151.3678, 184.6756, 180.5504, 196.6046, 141.5534]), Cost: 00.2745783925\n",
      "Epoch 9700/20000, hypothesis: tensor([151.3653, 184.6775, 180.5498, 196.6027, 141.5571]), Cost: 00.2737014890\n",
      "Epoch 9800/20000, hypothesis: tensor([151.3628, 184.6793, 180.5493, 196.6008, 141.5608]), Cost: 00.2728615403\n",
      "Epoch 9900/20000, hypothesis: tensor([151.3604, 184.6811, 180.5487, 196.5989, 141.5645]), Cost: 00.2720293999\n",
      "Epoch 10000/20000, hypothesis: tensor([151.3581, 184.6828, 180.5482, 196.5970, 141.5680]), Cost: 00.2712195218\n",
      "Epoch 10100/20000, hypothesis: tensor([151.3559, 184.6845, 180.5477, 196.5951, 141.5715]), Cost: 00.2704395950\n",
      "Epoch 10200/20000, hypothesis: tensor([151.3537, 184.6861, 180.5472, 196.5933, 141.5749]), Cost: 00.2696654201\n",
      "Epoch 10300/20000, hypothesis: tensor([151.3517, 184.6877, 180.5468, 196.5915, 141.5782]), Cost: 00.2689124048\n",
      "Epoch 10400/20000, hypothesis: tensor([151.3496, 184.6892, 180.5464, 196.5897, 141.5815]), Cost: 00.2681789994\n",
      "Epoch 10500/20000, hypothesis: tensor([151.3477, 184.6907, 180.5459, 196.5879, 141.5847]), Cost: 00.2674532533\n",
      "Epoch 10600/20000, hypothesis: tensor([151.3458, 184.6922, 180.5456, 196.5861, 141.5878]), Cost: 00.2667584419\n",
      "Epoch 10700/20000, hypothesis: tensor([151.3440, 184.6935, 180.5452, 196.5844, 141.5909]), Cost: 00.2660666108\n",
      "Epoch 10800/20000, hypothesis: tensor([151.3423, 184.6949, 180.5449, 196.5827, 141.5939]), Cost: 00.2653868198\n",
      "Epoch 10900/20000, hypothesis: tensor([151.3406, 184.6962, 180.5445, 196.5809, 141.5969]), Cost: 00.2647167444\n",
      "Epoch 11000/20000, hypothesis: tensor([151.3390, 184.6974, 180.5442, 196.5793, 141.5998]), Cost: 00.2640753686\n",
      "Epoch 11100/20000, hypothesis: tensor([151.3375, 184.6987, 180.5440, 196.5776, 141.6026]), Cost: 00.2634377778\n",
      "Epoch 11200/20000, hypothesis: tensor([151.3360, 184.6998, 180.5437, 196.5759, 141.6054]), Cost: 00.2628079355\n",
      "Epoch 11300/20000, hypothesis: tensor([151.3345, 184.7010, 180.5434, 196.5743, 141.6081]), Cost: 00.2621986270\n",
      "Epoch 11400/20000, hypothesis: tensor([151.3331, 184.7021, 180.5432, 196.5727, 141.6108]), Cost: 00.2615879476\n",
      "Epoch 11500/20000, hypothesis: tensor([151.3318, 184.7031, 180.5430, 196.5711, 141.6134]), Cost: 00.2609935403\n",
      "Epoch 11600/20000, hypothesis: tensor([151.3305, 184.7041, 180.5427, 196.5695, 141.6160]), Cost: 00.2604132295\n",
      "Epoch 11700/20000, hypothesis: tensor([151.3293, 184.7051, 180.5425, 196.5679, 141.6185]), Cost: 00.2598411143\n",
      "Epoch 11800/20000, hypothesis: tensor([151.3281, 184.7061, 180.5424, 196.5663, 141.6209]), Cost: 00.2592738271\n",
      "Epoch 11900/20000, hypothesis: tensor([151.3270, 184.7070, 180.5422, 196.5647, 141.6234]), Cost: 00.2587132454\n",
      "Epoch 12000/20000, hypothesis: tensor([151.3259, 184.7079, 180.5421, 196.5632, 141.6258]), Cost: 00.2581650019\n",
      "Epoch 12100/20000, hypothesis: tensor([151.3248, 184.7088, 180.5419, 196.5617, 141.6281]), Cost: 00.2576245368\n",
      "Epoch 12200/20000, hypothesis: tensor([151.3239, 184.7096, 180.5418, 196.5602, 141.6304]), Cost: 00.2570862174\n",
      "Epoch 12300/20000, hypothesis: tensor([151.3229, 184.7104, 180.5417, 196.5587, 141.6326]), Cost: 00.2565627098\n",
      "Epoch 12400/20000, hypothesis: tensor([151.3220, 184.7111, 180.5416, 196.5572, 141.6349]), Cost: 00.2560449839\n",
      "Epoch 12500/20000, hypothesis: tensor([151.3211, 184.7119, 180.5415, 196.5557, 141.6370]), Cost: 00.2555235028\n",
      "Epoch 12600/20000, hypothesis: tensor([151.3203, 184.7126, 180.5414, 196.5542, 141.6392]), Cost: 00.2550202608\n",
      "Epoch 12700/20000, hypothesis: tensor([151.3195, 184.7133, 180.5413, 196.5528, 141.6413]), Cost: 00.2545167804\n",
      "Epoch 12800/20000, hypothesis: tensor([151.3187, 184.7140, 180.5413, 196.5513, 141.6433]), Cost: 00.2540295720\n",
      "Epoch 12900/20000, hypothesis: tensor([151.3180, 184.7146, 180.5413, 196.5499, 141.6454]), Cost: 00.2535334527\n",
      "Epoch 13000/20000, hypothesis: tensor([151.3173, 184.7152, 180.5412, 196.5485, 141.6474]), Cost: 00.2530426681\n",
      "Epoch 13100/20000, hypothesis: tensor([151.3167, 184.7158, 180.5412, 196.5471, 141.6493]), Cost: 00.2525733113\n",
      "Epoch 13200/20000, hypothesis: tensor([151.3160, 184.7164, 180.5412, 196.5457, 141.6512]), Cost: 00.2521012127\n",
      "Epoch 13300/20000, hypothesis: tensor([151.3155, 184.7169, 180.5412, 196.5443, 141.6531]), Cost: 00.2516350746\n",
      "Epoch 13400/20000, hypothesis: tensor([151.3149, 184.7175, 180.5412, 196.5429, 141.6550]), Cost: 00.2511689067\n",
      "Epoch 13500/20000, hypothesis: tensor([151.3144, 184.7179, 180.5412, 196.5415, 141.6568]), Cost: 00.2507070005\n",
      "Epoch 13600/20000, hypothesis: tensor([151.3139, 184.7184, 180.5412, 196.5402, 141.6586]), Cost: 00.2502617836\n",
      "Epoch 13700/20000, hypothesis: tensor([151.3134, 184.7189, 180.5413, 196.5388, 141.6604]), Cost: 00.2498066872\n",
      "Epoch 13800/20000, hypothesis: tensor([151.3130, 184.7193, 180.5413, 196.5375, 141.6621]), Cost: 00.2493675202\n",
      "Epoch 13900/20000, hypothesis: tensor([151.3125, 184.7197, 180.5413, 196.5361, 141.6638]), Cost: 00.2489224970\n",
      "Epoch 14000/20000, hypothesis: tensor([151.3122, 184.7201, 180.5414, 196.5348, 141.6655]), Cost: 00.2484887838\n",
      "Epoch 14100/20000, hypothesis: tensor([151.3118, 184.7205, 180.5414, 196.5335, 141.6672]), Cost: 00.2480536997\n",
      "Epoch 14200/20000, hypothesis: tensor([151.3114, 184.7209, 180.5415, 196.5322, 141.6688]), Cost: 00.2476350367\n",
      "Epoch 14300/20000, hypothesis: tensor([151.3112, 184.7212, 180.5416, 196.5309, 141.6704]), Cost: 00.2471979111\n",
      "Epoch 14400/20000, hypothesis: tensor([151.3109, 184.7216, 180.5417, 196.5296, 141.6720]), Cost: 00.2467764616\n",
      "Epoch 14500/20000, hypothesis: tensor([151.3106, 184.7219, 180.5418, 196.5283, 141.6736]), Cost: 00.2463567704\n",
      "Epoch 14600/20000, hypothesis: tensor([151.3103, 184.7222, 180.5419, 196.5270, 141.6751]), Cost: 00.2459401786\n",
      "Epoch 14700/20000, hypothesis: tensor([151.3101, 184.7225, 180.5420, 196.5257, 141.6766]), Cost: 00.2455288172\n",
      "Epoch 14800/20000, hypothesis: tensor([151.3099, 184.7228, 180.5421, 196.5245, 141.6781]), Cost: 00.2451192141\n",
      "Epoch 14900/20000, hypothesis: tensor([151.3098, 184.7230, 180.5422, 196.5232, 141.6796]), Cost: 00.2447113991\n",
      "Epoch 15000/20000, hypothesis: tensor([151.3096, 184.7233, 180.5423, 196.5220, 141.6810]), Cost: 00.2443139106\n",
      "Epoch 15100/20000, hypothesis: tensor([151.3094, 184.7235, 180.5424, 196.5207, 141.6825]), Cost: 00.2439120114\n",
      "Epoch 15200/20000, hypothesis: tensor([151.3093, 184.7237, 180.5426, 196.5195, 141.6839]), Cost: 00.2435126305\n",
      "Epoch 15300/20000, hypothesis: tensor([151.3092, 184.7239, 180.5427, 196.5183, 141.6853]), Cost: 00.2431210279\n",
      "Epoch 15400/20000, hypothesis: tensor([151.3091, 184.7241, 180.5428, 196.5171, 141.6866]), Cost: 00.2427325547\n",
      "Epoch 15500/20000, hypothesis: tensor([151.3091, 184.7243, 180.5430, 196.5159, 141.6880]), Cost: 00.2423412353\n",
      "Epoch 15600/20000, hypothesis: tensor([151.3090, 184.7245, 180.5431, 196.5147, 141.6893]), Cost: 00.2419559956\n",
      "Epoch 15700/20000, hypothesis: tensor([151.3090, 184.7247, 180.5433, 196.5135, 141.6907]), Cost: 00.2415686101\n",
      "Epoch 15800/20000, hypothesis: tensor([151.3089, 184.7248, 180.5435, 196.5123, 141.6920]), Cost: 00.2411903888\n",
      "Epoch 15900/20000, hypothesis: tensor([151.3089, 184.7249, 180.5436, 196.5111, 141.6932]), Cost: 00.2408137023\n",
      "Epoch 16000/20000, hypothesis: tensor([151.3089, 184.7250, 180.5438, 196.5099, 141.6945]), Cost: 00.2404434681\n",
      "Epoch 16100/20000, hypothesis: tensor([151.3090, 184.7252, 180.5439, 196.5087, 141.6958]), Cost: 00.2400639504\n",
      "Epoch 16200/20000, hypothesis: tensor([151.3090, 184.7253, 180.5441, 196.5076, 141.6970]), Cost: 00.2396923304\n",
      "Epoch 16300/20000, hypothesis: tensor([151.3091, 184.7254, 180.5443, 196.5064, 141.6982]), Cost: 00.2393279076\n",
      "Epoch 16400/20000, hypothesis: tensor([151.3091, 184.7254, 180.5445, 196.5052, 141.6994]), Cost: 00.2389609814\n",
      "Epoch 16500/20000, hypothesis: tensor([151.3092, 184.7255, 180.5447, 196.5041, 141.7006]), Cost: 00.2386018038\n",
      "Epoch 16600/20000, hypothesis: tensor([151.3093, 184.7256, 180.5449, 196.5030, 141.7018]), Cost: 00.2382370979\n",
      "Epoch 16700/20000, hypothesis: tensor([151.3094, 184.7257, 180.5451, 196.5018, 141.7029]), Cost: 00.2378772497\n",
      "Epoch 16800/20000, hypothesis: tensor([151.3095, 184.7257, 180.5453, 196.5007, 141.7041]), Cost: 00.2375150174\n",
      "Epoch 16900/20000, hypothesis: tensor([151.3096, 184.7258, 180.5455, 196.4996, 141.7052]), Cost: 00.2371601611\n",
      "Epoch 17000/20000, hypothesis: tensor([151.3097, 184.7258, 180.5457, 196.4984, 141.7064]), Cost: 00.2368082553\n",
      "Epoch 17100/20000, hypothesis: tensor([151.3099, 184.7258, 180.5459, 196.4973, 141.7075]), Cost: 00.2364569902\n",
      "Epoch 17200/20000, hypothesis: tensor([151.3101, 184.7259, 180.5461, 196.4962, 141.7086]), Cost: 00.2361088991\n",
      "Epoch 17300/20000, hypothesis: tensor([151.3102, 184.7259, 180.5463, 196.4951, 141.7097]), Cost: 00.2357598990\n",
      "Epoch 17400/20000, hypothesis: tensor([151.3104, 184.7259, 180.5465, 196.4940, 141.7108]), Cost: 00.2354131639\n",
      "Epoch 17500/20000, hypothesis: tensor([151.3106, 184.7259, 180.5467, 196.4929, 141.7118]), Cost: 00.2350755632\n",
      "Epoch 17600/20000, hypothesis: tensor([151.3107, 184.7259, 180.5469, 196.4918, 141.7128]), Cost: 00.2347359657\n",
      "Epoch 17700/20000, hypothesis: tensor([151.3110, 184.7258, 180.5471, 196.4907, 141.7139]), Cost: 00.2343936414\n",
      "Epoch 17800/20000, hypothesis: tensor([151.3112, 184.7258, 180.5474, 196.4896, 141.7149]), Cost: 00.2340521812\n",
      "Epoch 17900/20000, hypothesis: tensor([151.3114, 184.7258, 180.5476, 196.4885, 141.7159]), Cost: 00.2337168902\n",
      "Epoch 18000/20000, hypothesis: tensor([151.3116, 184.7258, 180.5478, 196.4875, 141.7169]), Cost: 00.2333829701\n",
      "Epoch 18100/20000, hypothesis: tensor([151.3118, 184.7258, 180.5480, 196.4864, 141.7180]), Cost: 00.2330515385\n",
      "Epoch 18200/20000, hypothesis: tensor([151.3121, 184.7257, 180.5483, 196.4853, 141.7189]), Cost: 00.2327219993\n",
      "Epoch 18300/20000, hypothesis: tensor([151.3123, 184.7257, 180.5485, 196.4843, 141.7199]), Cost: 00.2323966920\n",
      "Epoch 18400/20000, hypothesis: tensor([151.3126, 184.7256, 180.5487, 196.4832, 141.7209]), Cost: 00.2320643216\n",
      "Epoch 18500/20000, hypothesis: tensor([151.3129, 184.7256, 180.5490, 196.4822, 141.7219]), Cost: 00.2317428887\n",
      "Epoch 18600/20000, hypothesis: tensor([151.3131, 184.7255, 180.5492, 196.4811, 141.7228]), Cost: 00.2314181030\n",
      "Epoch 18700/20000, hypothesis: tensor([151.3134, 184.7254, 180.5495, 196.4801, 141.7238]), Cost: 00.2310942411\n",
      "Epoch 18800/20000, hypothesis: tensor([151.3137, 184.7254, 180.5497, 196.4790, 141.7247]), Cost: 00.2307762653\n",
      "Epoch 18900/20000, hypothesis: tensor([151.3140, 184.7253, 180.5499, 196.4780, 141.7256]), Cost: 00.2304561436\n",
      "Epoch 19000/20000, hypothesis: tensor([151.3143, 184.7252, 180.5502, 196.4770, 141.7266]), Cost: 00.2301394045\n",
      "Epoch 19100/20000, hypothesis: tensor([151.3146, 184.7251, 180.5504, 196.4759, 141.7275]), Cost: 00.2298226058\n",
      "Epoch 19200/20000, hypothesis: tensor([151.3149, 184.7250, 180.5507, 196.4749, 141.7284]), Cost: 00.2295129597\n",
      "Epoch 19300/20000, hypothesis: tensor([151.3152, 184.7249, 180.5509, 196.4739, 141.7293]), Cost: 00.2292011231\n",
      "Epoch 19400/20000, hypothesis: tensor([151.3155, 184.7249, 180.5512, 196.4729, 141.7302]), Cost: 00.2288904637\n",
      "Epoch 19500/20000, hypothesis: tensor([151.3158, 184.7247, 180.5514, 196.4719, 141.7310]), Cost: 00.2285860777\n",
      "Epoch 19600/20000, hypothesis: tensor([151.3161, 184.7247, 180.5517, 196.4708, 141.7319]), Cost: 00.2282745540\n",
      "Epoch 19700/20000, hypothesis: tensor([151.3165, 184.7245, 180.5519, 196.4698, 141.7328]), Cost: 00.2279686183\n",
      "Epoch 19800/20000, hypothesis: tensor([151.3168, 184.7244, 180.5522, 196.4688, 141.7336]), Cost: 00.2276690453\n",
      "Epoch 19900/20000, hypothesis: tensor([151.3172, 184.7243, 180.5524, 196.4678, 141.7345]), Cost: 00.2273606807\n",
      "Epoch 20000/20000, hypothesis: tensor([151.3175, 184.7242, 180.5527, 196.4668, 141.7354]), Cost: 00.2270562947\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:04d}/{nb_epochs}, hypothesis: {hypothesis.squeeze().detach()}, Cost: {cost.item():013.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}